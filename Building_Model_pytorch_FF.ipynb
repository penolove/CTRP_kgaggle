{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec 0-0] Pytorch trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=np.random.normal(0, 0.001, (10,3))\n",
    "embedding.weight = nn.Parameter(torch.from_numpy(k))\n",
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.LongTensor([0,2,4,5]))\n",
    "input1 = Variable(torch.LongTensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = embedding(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## give a try at brocasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base * a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.t(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(torch.mm(torch.t(a),a))\n",
    "print(torch.mm(torch.t(a),a).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = torch.mm(torch.t(a),a).float()\n",
    "W = nn.Parameter(torch.ones(3,3))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(A)\n",
    "print(W*A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.mm(torch.t(a),a).trace()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec 0-1] Data Pre-processing - hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading data \n",
    "import pandas as pd\n",
    "X_train = pd.read_csv(\"data/train\")\n",
    "X_test = pd.read_csv(\"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add time column\n",
    "X_train['weekday']= (X_train['hour']%10000/100).astype(int)%7\n",
    "X_train['hour_']= (X_train['hour']%100)\n",
    "X_test['weekday']= (X_test['hour']%10000/100).astype(int)%7\n",
    "X_test['hour_']= (X_test['hour']%100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column2handle = list(X_test.columns.values)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(column2handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_list=[]\n",
    "# will be slowly as list grow up \n",
    "def get_hash_value(x):\n",
    "    try:\n",
    "        idx = hash_list.index(x)\n",
    "    except :\n",
    "        hash_list.append(x)\n",
    "        idx = len(hash_list)-1\n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict version\n",
    "hash_dict=dict()\n",
    "count =0 \n",
    "def get_hash_value_dict(x):\n",
    "    global count\n",
    "    idx = hash_dict.get(x,count)\n",
    "    if idx==count:\n",
    "        hash_dict[x]=count\n",
    "        count+=1\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column2handle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare 2 implement of hash method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "i='site_id'\n",
    "X_train[i+'_hash']=X_train[i].apply(get_hash_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "i='site_id'\n",
    "X_train[i+'_hash']=X_train[i].apply(get_hash_value_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obviously using dictionary is much suitable here\n",
    "## build up the hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict version\n",
    "hash_dict=dict()\n",
    "count =0 \n",
    "def get_hash_value_dict(x):\n",
    "    global count\n",
    "    idx = hash_dict.get(x,count)\n",
    "    if idx==count:\n",
    "        hash_dict[x]=count\n",
    "        count+=1\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in column2handle:\n",
    "    print (i)\n",
    "    X_train[i+'_hash']=X_train[i].apply(lambda x :get_hash_value_dict(i+'_'+str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"there are total : \",count,\" category in the training set\")\n",
    "print(count==len(hash_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in column2handle:\n",
    "    print(i)\n",
    "    X_test[i+'_hash']=X_test[i].apply(lambda x :get_hash_value_dict(i+'_'+str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"there are total : \",count,\" category in the training& testing set\")\n",
    "print(\"there are additional : \",count-9449236,\" category from testing set\")\n",
    "print(count==len(hash_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[['click']+[ i+'_hash' for i in column2handle]].to_pickle('train_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test[[ i+'_hash' for i in column2handle]].to_pickle('test_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save\n",
    "np.save('hash_dict.npy', hash_dict) \n",
    "\n",
    "# Load\n",
    "# read_dictionary = np.load('hash_dict.npy').item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec1] Building FF with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "X_train = pd.read_pickle('train_df_hash')\n",
    "X_test = pd.read_pickle('test_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we need a 10269200 embedding matrix, let k be 100\n",
    "embedding = nn.Embedding(10269200, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give a length 23 array\n",
    "input = Variable(torch.LongTensor(X_train.iloc[0,1:].values))\n",
    "# get the corresponds matrix\n",
    "W = embedding(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we want is \n",
    "$$\n",
    "\\phi_{FM}(w,x) =  \\Sigma_{j_1,j_2 \\epsilon C_{2}}   <w_{j1} , w_{j2}>  * x_{j1} * x_{j2}  -(1)\n",
    "$$\n",
    "$w_{j1}$and $w_{j2}$ are two vectors with length k\n",
    "\n",
    "and our objective function is :\n",
    "$$\n",
    "min_{w}  \\Sigma_{i=1}^{n} ( log(1+exp(-yi * \\phi(w,x_{i}) )) ) + \\frac{\\lambda}{2} * ||w||^{2} -(2) \n",
    "$$\n",
    "\n",
    "since C2 is a large number : 10269200\n",
    "\n",
    "for each sample i there acctually will be 23 $w_{j}$ will be involved in (1) \n",
    "\n",
    "\n",
    "now we extract the $w_{j}$ with corresponse sample i  have a matrix which is \n",
    "$$\n",
    "W_{i} = \\left(\\begin{array}{cc} \n",
    "w_{11} & ... w_{1k}\\\\\n",
    "w_{21} & ... w_{2k}\\\\\n",
    "...\\\\\n",
    "w_{f1} & ... w_{fk}\\\\\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "Let\n",
    "$$\n",
    "A = W_{i} * W_{i}'\n",
    "$$\n",
    "\n",
    "therefore\n",
    "$$\n",
    "\\phi_{FM}(w,x_{i}) =  (sum(A) + trace(A))/2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = torch.mm(W,torch.t(W))\n",
    "A.sum()+ A.trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['click'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec1-1] now make a nn.module for FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "X_train = pd.read_pickle('train_df_hash')\n",
    "X_test = pd.read_pickle('test_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "# create dataloader\n",
    "msk = np.random.rand(len(X_train)) < 0.8\n",
    "train_data = torch.LongTensor(X_train.iloc[msk,1:].values)\n",
    "train_labels = torch.from_numpy(X_train.iloc[msk,0].values).float()\n",
    "\n",
    "test_data = torch.LongTensor(X_train.iloc[~msk,1:].values)\n",
    "test_labels = torch.from_numpy(X_train.iloc[~msk,0].values).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create our dataloader first \n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "train = data_utils.TensorDataset(train_data, train_labels)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=1, shuffle=True,num_workers=8)\n",
    "\n",
    "test = data_utils.TensorDataset(test_data, test_labels)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=1, shuffle=False,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FF(nn.Module):\n",
    "    def __init__(self,k=50):\n",
    "        super(FF, self).__init__()\n",
    "        self.embedding = nn.Embedding(10269200, k, sparse=True)\n",
    "        init_mat = np.random.normal(0, 0.0001, (10269200,k)) # default embedding variance too large\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(init_mat).float())\n",
    "        self.bias = nn.Parameter(torch.ones(1)*0.18606) # let initial prob is close to 0.1698(mean of train click)\n",
    "        self.D = nn.Parameter(torch.ones(23,23))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, x):\n",
    "        W = self.embedding(x[0])\n",
    "        A = torch.mm(W,torch.t(W)) * self.D\n",
    "        A = self.dropout(A)\n",
    "        return (A.sum()+ A.trace()).view(-1, 1)+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableBCELoss(torch.nn.modules.Module):\n",
    "    def __init__(self):\n",
    "        super(StableBCELoss, self).__init__()\n",
    "    def forward(self, input, target):\n",
    "        neg_abs = - input.abs()\n",
    "        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "net = FF()\n",
    "net.cuda()\n",
    "criterion = StableBCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#net.load_state_dict(torch.load('FF_epoch{}.pth'.format(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10000] loss: 0.559\n",
      "[1, 20000] loss: 0.464\n",
      "[1, 30000] loss: 0.417\n",
      "[1, 40000] loss: 0.423\n",
      "[1, 50000] loss: 0.436\n",
      "[1, 60000] loss: 0.415\n",
      "[1, 70000] loss: 0.418\n",
      "[1, 80000] loss: 0.422\n",
      "[1, 90000] loss: 0.420\n",
      "[1, 100000] loss: 0.408\n",
      "[1, 110000] loss: 0.413\n",
      "[1, 120000] loss: 0.413\n",
      "[1, 130000] loss: 0.421\n",
      "[1, 140000] loss: 0.421\n",
      "[1, 150000] loss: 0.414\n",
      "[1, 160000] loss: 0.407\n",
      "[1, 170000] loss: 0.422\n",
      "[1, 180000] loss: 0.405\n",
      "[1, 190000] loss: 0.413\n",
      "[1, 200000] loss: 0.407\n",
      "[1, 210000] loss: 0.401\n",
      "[1, 220000] loss: 0.418\n",
      "[1, 230000] loss: 0.414\n",
      "[1, 240000] loss: 0.419\n",
      "[1, 250000] loss: 0.401\n",
      "[1, 260000] loss: 0.401\n",
      "[1, 270000] loss: 0.405\n",
      "[1, 280000] loss: 0.404\n",
      "[1, 290000] loss: 0.401\n",
      "[1, 300000] loss: 0.417\n",
      "[1, 310000] loss: 0.401\n",
      "[1, 320000] loss: 0.412\n",
      "[1, 330000] loss: 0.400\n",
      "[1, 340000] loss: 0.415\n",
      "[1, 350000] loss: 0.408\n",
      "[1, 360000] loss: 0.406\n",
      "[1, 370000] loss: 0.399\n",
      "[1, 380000] loss: 0.413\n",
      "[1, 390000] loss: 0.403\n",
      "[1, 400000] loss: 0.408\n",
      "[1, 410000] loss: 0.401\n",
      "[1, 420000] loss: 0.405\n",
      "[1, 430000] loss: 0.401\n",
      "[1, 440000] loss: 0.402\n",
      "[1, 450000] loss: 0.401\n",
      "[1, 460000] loss: 0.405\n",
      "[1, 470000] loss: 0.410\n",
      "[1, 480000] loss: 0.410\n",
      "[1, 490000] loss: 0.407\n",
      "[1, 500000] loss: 0.410\n",
      "[1, 510000] loss: 0.390\n",
      "[1, 520000] loss: 0.394\n",
      "[1, 530000] loss: 0.403\n",
      "[1, 540000] loss: 0.403\n",
      "[1, 550000] loss: 0.397\n",
      "[1, 560000] loss: 0.404\n",
      "[1, 570000] loss: 0.413\n",
      "[1, 580000] loss: 0.404\n",
      "[1, 590000] loss: 0.410\n",
      "[1, 600000] loss: 0.398\n",
      "[1, 610000] loss: 0.409\n",
      "[1, 620000] loss: 0.399\n",
      "[1, 630000] loss: 0.402\n",
      "[1, 640000] loss: 0.397\n",
      "[1, 650000] loss: 0.408\n",
      "[1, 660000] loss: 0.397\n",
      "[1, 670000] loss: 0.406\n",
      "[1, 680000] loss: 0.404\n",
      "[1, 690000] loss: 0.401\n",
      "[1, 700000] loss: 0.400\n",
      "[1, 710000] loss: 0.401\n",
      "[1, 720000] loss: 0.396\n",
      "[1, 730000] loss: 0.410\n",
      "[1, 740000] loss: 0.405\n",
      "[1, 750000] loss: 0.409\n",
      "[1, 760000] loss: 0.402\n",
      "[1, 770000] loss: 0.401\n",
      "[1, 780000] loss: 0.396\n",
      "[1, 790000] loss: 0.397\n",
      "[1, 800000] loss: 0.410\n",
      "[1, 810000] loss: 0.402\n",
      "[1, 820000] loss: 0.397\n",
      "[1, 830000] loss: 0.416\n",
      "[1, 840000] loss: 0.399\n",
      "[1, 850000] loss: 0.404\n",
      "[1, 860000] loss: 0.392\n",
      "[1, 870000] loss: 0.409\n",
      "[1, 880000] loss: 0.400\n",
      "[1, 890000] loss: 0.403\n",
      "[1, 900000] loss: 0.401\n",
      "[1, 910000] loss: 0.410\n",
      "[1, 920000] loss: 0.409\n",
      "[1, 930000] loss: 0.406\n",
      "[1, 940000] loss: 0.412\n",
      "[1, 950000] loss: 0.401\n",
      "[1, 960000] loss: 0.389\n",
      "[1, 970000] loss: 0.400\n",
      "[1, 980000] loss: 0.408\n",
      "[1, 990000] loss: 0.399\n",
      "[1, 1000000] loss: 0.400\n",
      "[1, 1010000] loss: 0.396\n",
      "[1, 1020000] loss: 0.403\n",
      "[1, 1030000] loss: 0.397\n",
      "[1, 1040000] loss: 0.389\n",
      "[1, 1050000] loss: 0.402\n",
      "[1, 1060000] loss: 0.400\n",
      "[1, 1070000] loss: 0.404\n",
      "[1, 1080000] loss: 0.405\n",
      "[1, 1090000] loss: 0.395\n",
      "[1, 1100000] loss: 0.399\n",
      "[1, 1110000] loss: 0.409\n",
      "[1, 1120000] loss: 0.398\n",
      "[1, 1130000] loss: 0.413\n",
      "[1, 1140000] loss: 0.397\n",
      "[1, 1150000] loss: 0.388\n",
      "[1, 1160000] loss: 0.403\n",
      "[1, 1170000] loss: 0.401\n",
      "[1, 1180000] loss: 0.404\n",
      "[1, 1190000] loss: 0.389\n",
      "[1, 1200000] loss: 0.402\n",
      "[1, 1210000] loss: 0.399\n",
      "[1, 1220000] loss: 0.402\n",
      "[1, 1230000] loss: 0.392\n",
      "[1, 1240000] loss: 0.405\n",
      "[1, 1250000] loss: 0.400\n"
     ]
    }
   ],
   "source": [
    "min_testing_loss = 100\n",
    "#min_testing_loss = 0\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.float().cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 10000 == 9999:    # print every 10000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10000))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    #============= eval testing ================\n",
    "    running_loss = 0.0\n",
    "    count=0\n",
    "    net.eval()\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        count+=1\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.float().cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "    testing_loss = running_loss / count\n",
    "    print('[%d, %5d] testing loss: %.3f' %(epoch + 1, count, testing_loss))\n",
    "    if min_testing_loss>testing_loss:\n",
    "        torch.save(net.state_dict(), 'FF_epoch{}_{}.pth'.format(epoch,testing_loss))\n",
    "        min_testing_loss = testing_loss\n",
    "    running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'FF_epoch{}_{}.pth'.format(1,min_testing_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[1, 8084727] testing loss: 0.375\n",
    "\n",
    "# torch.save(net.state_dict(), 'FF_epoch{}_{}.pth'.format(1,0.373))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec1-2] apply on testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob(x):\n",
    "    a=np.exp(x)\n",
    "    return a/(1+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#net.load_state_dict(torch.load('FF_epoch{}.pth'.format(1)))\n",
    "\n",
    "A = np.array([])\n",
    "for i in range(len(X_test)):\n",
    "    input = X_test.values[i]\n",
    "    input = Variable(torch.LongTensor(input))\n",
    "    input = input.contiguous().view(1,-1).cuda()\n",
    "    out = net(input)\n",
    "    prob_out = prob(out.data.cpu().numpy()[0][0])\n",
    "    A = np.append(A, prob_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sampleSubmission = pd.read_csv(\"data/sampleSubmission\")\n",
    "sampleSubmission.click = A\n",
    "# with a naive feature hashing (0.408, 1142/1604)\n",
    "sampleSubmission.to_csv(\"FF_pytorch_20170826.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec2] Build a model for Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with using torch.bmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(torch.mm(torch.t(a),a))\n",
    "print(torch.mm(torch.t(a),a).sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
