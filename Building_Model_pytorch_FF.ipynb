{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec 0-0] Pytorch trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=np.random.normal(0, 0.001, (10,3))\n",
    "embedding.weight = nn.Parameter(torch.from_numpy(k))\n",
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.LongTensor([0,2,4,5]))\n",
    "input1 = Variable(torch.LongTensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = embedding(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## give a try at brocasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base * a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.t(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(torch.mm(torch.t(a),a))\n",
    "print(torch.mm(torch.t(a),a).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = torch.mm(torch.t(a),a).float()\n",
    "W = nn.Parameter(torch.ones(3,3))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(A)\n",
    "print(W*A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.mm(torch.t(a),a).trace()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec 0-1] Data Pre-processing - hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading data \n",
    "import pandas as pd\n",
    "X_train = pd.read_csv(\"data/train.csv\")\n",
    "X_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add time column\n",
    "X_train['weekday']= (X_train['hour']%10000/100).astype(int)%7\n",
    "X_train['hour_']= (X_train['hour']%100)\n",
    "X_test['weekday']= (X_test['hour']%10000/100).astype(int)%7\n",
    "X_test['hour_']= (X_test['hour']%100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column2handle = list(X_test.columns.values)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C1', 'banner_pos', 'site_id', 'site_domain', 'site_category', 'app_id', 'app_domain', 'app_category', 'device_id', 'device_ip', 'device_model', 'device_type', 'device_conn_type', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'weekday', 'hour_']\n"
     ]
    }
   ],
   "source": [
    "print(column2handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_list=[]\n",
    "# will be slowly as list grow up \n",
    "def get_hash_value(x):\n",
    "    try:\n",
    "        idx = hash_list.index(x)\n",
    "    except :\n",
    "        hash_list.append(x)\n",
    "        idx = len(hash_list)-1\n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict version\n",
    "hash_dict=dict()\n",
    "count =0 \n",
    "def get_hash_value_dict(x):\n",
    "    global count\n",
    "    idx = hash_dict.get(x,count)\n",
    "    if idx==count:\n",
    "        hash_dict[x]=count\n",
    "        count+=1\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column2handle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'click', 'hour', 'C1', 'banner_pos', 'site_id', 'site_domain',\n",
       "       'site_category', 'app_id', 'app_domain', 'app_category',\n",
       "       'device_id', 'device_ip', 'device_model', 'device_type',\n",
       "       'device_conn_type', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20',\n",
       "       'C21', 'weekday', 'hour_'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare 2 implement of hash method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 34s, sys: 72 ms, total: 2min 34s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i='site_id'\n",
    "X_train[i+'_hash']=X_train[i].apply(get_hash_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 48 ms, total: 13.9 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i='site_id'\n",
    "X_train[i+'_hash']=X_train[i].apply(get_hash_value_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obviously using dictionary is much suitable here\n",
    "## build up the hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict version\n",
    "hash_dict=dict()\n",
    "count =0 \n",
    "def get_hash_value_dict(x):\n",
    "    global count\n",
    "    idx = hash_dict.get(x,count)\n",
    "    if idx==count:\n",
    "        hash_dict[x]=count\n",
    "        count+=1\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1\n",
      "banner_pos\n",
      "site_id\n",
      "site_domain\n",
      "site_category\n",
      "app_id\n",
      "app_domain\n",
      "app_category\n",
      "device_id\n",
      "device_ip\n",
      "device_model\n",
      "device_type\n",
      "device_conn_type\n",
      "C14\n",
      "C15\n",
      "C16\n",
      "C17\n",
      "C18\n",
      "C19\n",
      "C20\n",
      "C21\n",
      "weekday\n",
      "hour_\n",
      "CPU times: user 10min 5s, sys: 3.62 s, total: 10min 9s\n",
      "Wall time: 10min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in column2handle:\n",
    "    print (i)\n",
    "    X_train[i+'_hash']=X_train[i].apply(lambda x :get_hash_value_dict(i+'_'+str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are total :  9449236  category in the training set\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"there are total : \",count,\" category in the training set\")\n",
    "print(count==len(hash_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1\n",
      "banner_pos\n",
      "site_id\n",
      "site_domain\n",
      "site_category\n",
      "app_id\n",
      "app_domain\n",
      "app_category\n",
      "device_id\n",
      "device_ip\n",
      "device_model\n",
      "device_type\n",
      "device_conn_type\n",
      "C14\n",
      "C15\n",
      "C16\n",
      "C17\n",
      "C18\n",
      "C19\n",
      "C20\n",
      "C21\n",
      "weekday\n",
      "hour_\n",
      "CPU times: user 1min 8s, sys: 148 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in column2handle:\n",
    "    print(i)\n",
    "    X_test[i+'_hash']=X_test[i].apply(lambda x :get_hash_value_dict(i+'_'+str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are total :  10269200  category in the training& testing set\n",
      "there are additional :  819964  category from testing set\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"there are total : \",count,\" category in the training& testing set\")\n",
    "print(\"there are additional : \",count-9449236,\" category from testing set\")\n",
    "print(count==len(hash_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[['click']+[ i+'_hash' for i in column2handle]].to_pickle('train_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test[[ i+'_hash' for i in column2handle]].to_pickle('test_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save\n",
    "np.save('hash_dict.npy', hash_dict) \n",
    "\n",
    "# Load\n",
    "# read_dictionary = np.load('hash_dict.npy').item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec1] Building FF with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "X_train = pd.read_pickle('train_df_hash')\n",
    "X_test = pd.read_pickle('test_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we need a 10269200 embedding matrix, let k be 100\n",
    "embedding = nn.Embedding(10269200, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give a length 23 array\n",
    "input = Variable(torch.LongTensor(X_train.iloc[0,1:].values))\n",
    "# get the corresponds matrix\n",
    "W = embedding(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we want is \n",
    "$$\n",
    "\\phi_{FM}(w,x) =  \\Sigma_{j_1,j_2 \\epsilon C_{2}}   <w_{j1} , w_{j2}>  * x_{j1} * x_{j2}  -(1)\n",
    "$$\n",
    "$w_{j1}$and $w_{j2}$ are two vectors with length k\n",
    "\n",
    "and our objective function is :\n",
    "$$\n",
    "min_{w}  \\Sigma_{i=1}^{n} ( log(1+exp(-yi * \\phi(w,x_{i}) )) ) + \\frac{\\lambda}{2} * ||w||^{2} -(2) \n",
    "$$\n",
    "\n",
    "since C2 is a large number : 10269200\n",
    "\n",
    "for each sample i there acctually will be 23 $w_{j}$ will be involved in (1) \n",
    "\n",
    "\n",
    "now we extract the $w_{j}$ with corresponse sample i  have a matrix which is \n",
    "$$\n",
    "W_{i} = \\left(\\begin{array}{cc} \n",
    "w_{11} & ... w_{1k}\\\\\n",
    "w_{21} & ... w_{2k}\\\\\n",
    "...\\\\\n",
    "w_{f1} & ... w_{fk}\\\\\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "Let\n",
    "$$\n",
    "A = W_{i} * W_{i}'\n",
    "$$\n",
    "\n",
    "therefore\n",
    "$$\n",
    "\\phi_{FM}(w,x_{i}) =  (sum(A) + trace(A))/2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = torch.mm(W,torch.t(W))\n",
    "A.sum()+ A.trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['click'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec1-1] now make a nn.module for FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "X_train = pd.read_pickle('train_df_hash')\n",
    "X_test = pd.read_pickle('test_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "# create dataloader\n",
    "msk = np.random.rand(len(X_train)) < 0.8\n",
    "train_data = torch.LongTensor(X_train.iloc[msk,1:].values)\n",
    "train_labels = torch.from_numpy(X_train.iloc[msk,0].values).float()\n",
    "\n",
    "test_data = torch.LongTensor(X_train.iloc[~msk,1:].values)\n",
    "test_labels = torch.from_numpy(X_train.iloc[~msk,0].values).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create our dataloader first \n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "train = data_utils.TensorDataset(train_data, train_labels)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=1, shuffle=True,num_workers=8)\n",
    "\n",
    "test = data_utils.TensorDataset(test_data, test_labels)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=1, shuffle=False,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FF(nn.Module):\n",
    "    def __init__(self,k=50):\n",
    "        super(FF, self).__init__()\n",
    "        self.embedding = nn.Embedding(10269200, k, sparse=True)\n",
    "        init_mat = np.random.normal(0, 0.0001, (10269200,k)) # default embedding variance too large\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(init_mat).float())\n",
    "        self.bias = nn.Parameter(torch.ones(1)*0.18606) # let initial prob is close to 0.1698(mean of train click)\n",
    "        self.D = nn.Parameter(torch.ones(23,23))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, x):\n",
    "        W = self.embedding(x[0])\n",
    "        A = torch.mm(W,torch.t(W)) * self.D\n",
    "        A = self.dropout(A)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StableBCELoss(torch.nn.modules.Module):\n",
    "    def __init__(self):\n",
    "        super(StableBCELoss, self).__init__()\n",
    "    def forward(self, input, target,alpha=0,l2_term=0):\n",
    "        neg_abs = - input.abs()\n",
    "        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
    "        if(alpha>0):\n",
    "            return loss.mean()+alpha*l2_term\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "net = FF()\n",
    "net.cuda()\n",
    "criterion = StableBCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#net.load_state_dict(torch.load('FF_epoch{}.pth'.format(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1230000] loss: 0.332\n",
      "[2, 1240000] loss: 0.333\n",
      "[2, 1250000] loss: 0.335\n",
      "[2, 1260000] loss: 0.326\n",
      "[2, 1270000] loss: 0.341\n",
      "[2, 1280000] loss: 0.331\n",
      "[2, 1290000] loss: 0.343\n",
      "[2, 1300000] loss: 0.334\n",
      "[2, 1310000] loss: 0.331\n",
      "[2, 1320000] loss: 0.331\n",
      "[2, 1330000] loss: 0.330\n",
      "[2, 1340000] loss: 0.333\n",
      "[2, 1350000] loss: 0.340\n",
      "[2, 1360000] loss: 0.325\n",
      "[2, 1370000] loss: 0.324\n",
      "[2, 1380000] loss: 0.328\n",
      "[2, 1390000] loss: 0.322\n",
      "[2, 1400000] loss: 0.320\n",
      "[2, 1410000] loss: 0.331\n",
      "[2, 1420000] loss: 0.329\n",
      "[2, 1430000] loss: 0.333\n",
      "[2, 1440000] loss: 0.339\n",
      "[2, 1450000] loss: 0.327\n",
      "[2, 1460000] loss: 0.330\n",
      "[2, 1470000] loss: 0.338\n",
      "[2, 1480000] loss: 0.329\n",
      "[2, 1490000] loss: 0.331\n",
      "[2, 1500000] loss: 0.331\n",
      "[2, 1510000] loss: 0.328\n",
      "[2, 1520000] loss: 0.330\n",
      "[2, 1530000] loss: 0.324\n",
      "[2, 1540000] loss: 0.329\n",
      "[2, 1550000] loss: 0.325\n",
      "[2, 1560000] loss: 0.329\n",
      "[2, 1570000] loss: 0.334\n",
      "[2, 1580000] loss: 0.333\n",
      "[2, 1590000] loss: 0.334\n",
      "[2, 1600000] loss: 0.326\n",
      "[2, 1610000] loss: 0.325\n",
      "[2, 1620000] loss: 0.335\n",
      "[2, 1630000] loss: 0.325\n",
      "[2, 1640000] loss: 0.333\n",
      "[2, 1650000] loss: 0.326\n",
      "[2, 1660000] loss: 0.323\n",
      "[2, 1670000] loss: 0.329\n",
      "[2, 1680000] loss: 0.334\n",
      "[2, 1690000] loss: 0.336\n",
      "[2, 1700000] loss: 0.339\n",
      "[2, 1710000] loss: 0.327\n",
      "[2, 1720000] loss: 0.325\n",
      "[2, 1730000] loss: 0.332\n",
      "[2, 1740000] loss: 0.330\n",
      "[2, 1750000] loss: 0.321\n",
      "[2, 1760000] loss: 0.327\n",
      "[2, 1770000] loss: 0.329\n",
      "[2, 1780000] loss: 0.323\n",
      "[2, 1790000] loss: 0.322\n",
      "[2, 1800000] loss: 0.324\n",
      "[2, 1810000] loss: 0.336\n",
      "[2, 1820000] loss: 0.330\n",
      "[2, 1830000] loss: 0.331\n",
      "[2, 1840000] loss: 0.332\n",
      "[2, 1850000] loss: 0.329\n",
      "[2, 1860000] loss: 0.328\n",
      "[2, 1870000] loss: 0.334\n",
      "[2, 1880000] loss: 0.324\n",
      "[2, 1890000] loss: 0.333\n",
      "[2, 1900000] loss: 0.325\n",
      "[2, 1910000] loss: 0.319\n",
      "[2, 1920000] loss: 0.326\n",
      "[2, 1930000] loss: 0.326\n",
      "[2, 1940000] loss: 0.335\n",
      "[2, 1950000] loss: 0.325\n",
      "[2, 1960000] loss: 0.330\n",
      "[2, 1970000] loss: 0.325\n",
      "[2, 1980000] loss: 0.324\n",
      "[2, 1990000] loss: 0.331\n",
      "[2, 2000000] loss: 0.329\n",
      "[2, 2010000] loss: 0.324\n",
      "[2, 2020000] loss: 0.327\n",
      "[2, 2030000] loss: 0.321\n",
      "[2, 2040000] loss: 0.333\n",
      "[2, 2050000] loss: 0.332\n",
      "[2, 2060000] loss: 0.341\n",
      "[2, 2070000] loss: 0.324\n",
      "[2, 2080000] loss: 0.339\n",
      "[2, 2090000] loss: 0.325\n",
      "[2, 2100000] loss: 0.333\n",
      "[2, 2110000] loss: 0.333\n",
      "[2, 2120000] loss: 0.328\n",
      "[2, 2130000] loss: 0.333\n",
      "[2, 2140000] loss: 0.326\n",
      "[2, 2150000] loss: 0.335\n",
      "[2, 2160000] loss: 0.332\n",
      "[2, 2170000] loss: 0.331\n",
      "[2, 2180000] loss: 0.327\n",
      "[2, 2190000] loss: 0.321\n",
      "[2, 2200000] loss: 0.330\n",
      "[2, 2210000] loss: 0.331\n",
      "[2, 2220000] loss: 0.330\n",
      "[2, 2230000] loss: 0.323\n",
      "[2, 2240000] loss: 0.325\n",
      "[2, 2250000] loss: 0.324\n",
      "[2, 2260000] loss: 0.331\n",
      "[2, 2270000] loss: 0.318\n",
      "[2, 2280000] loss: 0.328\n",
      "[2, 2290000] loss: 0.322\n",
      "[2, 2300000] loss: 0.331\n",
      "[2, 2310000] loss: 0.327\n",
      "[2, 2320000] loss: 0.334\n",
      "[2, 2330000] loss: 0.326\n",
      "[2, 2340000] loss: 0.325\n",
      "[2, 2350000] loss: 0.327\n",
      "[2, 2360000] loss: 0.332\n",
      "[2, 2370000] loss: 0.326\n",
      "[2, 2380000] loss: 0.334\n",
      "[2, 2390000] loss: 0.319\n",
      "[2, 2400000] loss: 0.333\n",
      "[2, 2410000] loss: 0.332\n",
      "[2, 2420000] loss: 0.312\n",
      "[2, 2430000] loss: 0.332\n",
      "[2, 2440000] loss: 0.332\n",
      "[2, 2450000] loss: 0.329\n",
      "[2, 2460000] loss: 0.319\n",
      "[2, 2470000] loss: 0.329\n",
      "[2, 2480000] loss: 0.327\n",
      "[2, 2490000] loss: 0.319\n",
      "[2, 2500000] loss: 0.329\n",
      "[2, 2510000] loss: 0.326\n",
      "[2, 2520000] loss: 0.330\n",
      "[2, 2530000] loss: 0.330\n",
      "[2, 2540000] loss: 0.335\n",
      "[2, 2550000] loss: 0.326\n",
      "[2, 2560000] loss: 0.326\n",
      "[2, 2570000] loss: 0.317\n",
      "[2, 2580000] loss: 0.324\n",
      "[2, 2590000] loss: 0.318\n",
      "[2, 2600000] loss: 0.320\n",
      "[2, 2610000] loss: 0.327\n",
      "[2, 2620000] loss: 0.329\n",
      "[2, 2630000] loss: 0.323\n",
      "[2, 2640000] loss: 0.321\n",
      "[2, 2650000] loss: 0.318\n",
      "[2, 2660000] loss: 0.327\n",
      "[2, 2670000] loss: 0.338\n",
      "[2, 2680000] loss: 0.334\n",
      "[2, 2690000] loss: 0.327\n",
      "[2, 2700000] loss: 0.324\n",
      "[2, 2710000] loss: 0.325\n",
      "[2, 2720000] loss: 0.332\n",
      "[2, 2730000] loss: 0.324\n",
      "[2, 2740000] loss: 0.334\n",
      "[2, 2750000] loss: 0.325\n",
      "[2, 2760000] loss: 0.319\n",
      "[2, 2770000] loss: 0.327\n",
      "[2, 2780000] loss: 0.325\n",
      "[2, 2790000] loss: 0.331\n",
      "[2, 2800000] loss: 0.329\n",
      "[2, 2810000] loss: 0.326\n",
      "[2, 2820000] loss: 0.333\n",
      "[2, 2830000] loss: 0.329\n"
     ]
    }
   ],
   "source": [
    "min_testing_loss = 100\n",
    "#min_testing_loss = 0\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.float().cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        A = net(inputs)\n",
    "        outputs = (A.sum()+ A.trace()).view(-1, 1)+net.bias\n",
    "        loss = criterion(outputs, labels.view(-1, 1),1e-3,A.trace())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 10000 == 9999:    # print every 10000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10000))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    #============= eval testing ================\n",
    "    running_loss = 0.0\n",
    "    count=0\n",
    "    net.eval()\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        count+=1\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.float().cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        A = net(inputs)\n",
    "        outputs = (A.sum()+ A.trace()).view(-1, 1)+net.bias\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "    testing_loss = running_loss / count\n",
    "    print('[%d, %5d] testing loss: %.3f' %(epoch + 1, count, testing_loss))\n",
    "    if min_testing_loss>testing_loss:\n",
    "        torch.save(net.state_dict(), 'FF_epoch{}_{}.pth'.format(epoch,testing_loss))\n",
    "        min_testing_loss = testing_loss\n",
    "    running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[1, 8083436] testing loss: 0.375\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'FF_epoch{}_{}.pth'.format(1,min_testing_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[1, 8084727] testing loss: 0.375\n",
    "\n",
    "# torch.save(net.state_dict(), 'FF_epoch{}_{}.pth'.format(1,0.373))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec1-2] apply on testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob(x):\n",
    "    a=np.exp(x)\n",
    "    return a/(1+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#net.load_state_dict(torch.load('FF_epoch{}.pth'.format(1)))\n",
    "\n",
    "A = np.array([])\n",
    "for i in range(len(X_test)):\n",
    "    input = X_test.values[i]\n",
    "    input = Variable(torch.LongTensor(input))\n",
    "    input = input.contiguous().view(1,-1).cuda()\n",
    "    out = net(input)\n",
    "    prob_out = prob(out.data.cpu().numpy()[0][0])\n",
    "    A = np.append(A, prob_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sampleSubmission = pd.read_csv(\"data/sampleSubmission\")\n",
    "sampleSubmission.click = A\n",
    "# with a naive feature hashing (0.408, 1142/1604)\n",
    "sampleSubmission.to_csv(\"FF_pytorch_20170826.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec2] Build a model for Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with using torch.bmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(torch.mm(torch.t(a),a))\n",
    "print(torch.mm(torch.t(a),a).sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
