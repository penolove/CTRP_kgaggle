{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sec1] create Sparse Matrix with pd get_dummies (failure)\n",
    "\n",
    "fail due to >pd.get_dummies is extremely unefficientcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X_train = pd.read_csv(\"data/train.csv\")\n",
    "X_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=X_test.columns.values[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling col :  C1\n",
      "handling col :  banner_pos\n",
      "handling col :  site_id\n",
      "handling col :  site_domain\n",
      "handling col :  site_category\n",
      "handling col :  app_id\n",
      "handling col :  app_domain\n",
      "handling col :  app_category\n",
      "handling col :  device_id\n",
      "handling col :  device_ip\n",
      "handling col :  device_model\n",
      "handling col :  device_type\n",
      "handling col :  device_conn_type\n",
      "handling col :  C14\n",
      "handling col :  C15\n",
      "handling col :  C16\n",
      "handling col :  C17\n",
      "handling col :  C18\n",
      "handling col :  C19\n",
      "handling col :  C20\n",
      "handling col :  C21\n"
     ]
    }
   ],
   "source": [
    "features = X_test.columns.values[2:]\n",
    "for column in features:\n",
    "    print(\"handling col : \",column )\n",
    "    categories = pd.concat([X_train[column],X_test[column]]).unique()\n",
    "    X_train[column] = X_train[column].astype('category', categories=categories)\n",
    "    X_test[column] = X_test[column].astype('category', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"Train_cate\")\n",
    "X_test.to_pickle(\"Test_cate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=pd.read_pickle(\"Train_cate\")\n",
    "X_test=pd.read_pickle(\"Test_cate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into 5 folds\n",
    "import numpy as np\n",
    "def n_fold_split(df, k, seed=5566):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df)\n",
    "    fold_size = int(m/k)\n",
    "    fold_list = []\n",
    "    for i in range(k):\n",
    "        if i!=(k-1):\n",
    "            target_range=range(i*fold_size,(i+1)*fold_size)\n",
    "        else:\n",
    "            target_range=range(i*fold_size,m)\n",
    "        current_df = df.iloc[perm[target_range]]\n",
    "        fold_list.append(current_df)\n",
    "    return fold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085795\n"
     ]
    }
   ],
   "source": [
    "X_train_list = n_fold_split(X_train,5)\n",
    "for i in X_train_list:\n",
    "    print(\"batchsize:\",len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=X_test.columns.values[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C1', 'banner_pos', 'site_id', 'site_domain', 'site_category',\n",
       "       'app_id', 'app_domain', 'app_category', 'device_id', 'device_ip',\n",
       "       'device_model', 'device_type', 'device_conn_type', 'C14', 'C15',\n",
       "       'C16', 'C17', 'C18', 'C19', 'C20', 'C21'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.get_dummies(X_test, columns=features, sparse=True)\n",
    "# extremely slowly and run-out memory (even with only Test dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec2] manually build csr matrix, and fit xgb/SGD model\n",
    "what we want -> a very large matrix with all category variable were translated to dummy\n",
    "\n",
    "and sloving with logistic regression\n",
    "\n",
    "## [Sec2-1]   load df which were built in FF ipynb,  and split it into 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "X_train = pd.read_pickle('train_df_hash')\n",
    "X_test = pd.read_pickle('test_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column2handle = X_train.columns.values[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split to 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 5 folds\n",
    "import numpy as np\n",
    "def n_fold_split(df, k, seed=5566,shuffle=True):\n",
    "    np.random.seed(seed)\n",
    "    if(shuffle):\n",
    "        perm = np.random.permutation(df.index)\n",
    "    else:\n",
    "        perm= np.arange(len(df))\n",
    "    m = len(df)\n",
    "    fold_size = int(m/k)\n",
    "    fold_list = []\n",
    "    for i in range(k):\n",
    "        if i!=(k-1):\n",
    "            target_range=range(i*fold_size,(i+1)*fold_size)\n",
    "        else:\n",
    "            target_range=range(i*fold_size,m)\n",
    "        current_df = df.iloc[perm[target_range]]\n",
    "        fold_list.append(current_df)\n",
    "    return fold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085795\n"
     ]
    }
   ],
   "source": [
    "X_train_list = n_fold_split(X_train,5,shuffle=False)\n",
    "for i in X_train_list:\n",
    "    print(\"batchsize:\",len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Sec 2-1-0] Test the format of sparse matrix\n",
    "said dok_matrix/lil_matrix is good at constructing sparse matrices.\n",
    "\n",
    "give a naive try to this two type and save it as csr_matrix\n",
    "\n",
    "[REF](http://m.blog.csdn.net/pipisorry/article/details/41762945)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on dok_matrix\n",
    "X_train_dok = dok_matrix((8085793, 10269200), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 51s, sys: 6.21 s, total: 8min 57s\n",
      "Wall time: 9min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in column2handle:\n",
    "    target = X_train_list[0][i]\n",
    "    X_train_dok[range(8085793),target.values]=1\n",
    "X_train_dok = X_train_dok.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on lil_matrix\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lil = lil_matrix((8085793, 10269200), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.6 s, sys: 6.13 s, total: 1min\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in column2handle:\n",
    "    target = X_train_list[0][i]\n",
    "    X_train_lil[range(8085793),target.values]=1\n",
    "X_train_lil = X_train_lil.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7421"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [sec 2-1-1] filling hash values into sparse matrix \n",
    "### using lil_matrix is much more faster for assigning values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save/load function for  csr_matrix\n",
    "from scipy.sparse import csr_matrix,lil_matrix\n",
    "import numpy as np\n",
    "import gc\n",
    "def save_sparse_csr(filename, array):\n",
    "    # note that .npz extension is added automatically\n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    # here we need to add .npz extension manually\n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "CPU times: user 8min 48s, sys: 50 s, total: 9min 38s\n",
      "Wall time: 9min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for j in range(5):\n",
    "    X_train_lil = lil_matrix(( len(X_train_list[j]), 10269200), dtype=np.bool)\n",
    "    gc.collect()\n",
    "    for i in column2handle:\n",
    "        print(i)\n",
    "        target = X_train_list[j][i]\n",
    "        X_train_lil[range(len(X_train_list[j])),target.values]=1\n",
    "        \n",
    "    X_train_lil = X_train_lil.tocsr()\n",
    "    save_sparse_csr('data/X_train_csr_shuff_'+str(j), X_train_lil)\n",
    "    np.save('data/X_train_csr_click_shuff_'+str(j), X_train_list[j].click.values)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ba2e6b2e4775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX_test_lil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_lil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msave_sparse_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/X_test_csr_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_lil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "# X_test \n",
    "X_test_lil = lil_matrix(( len(X_test), 10269200), dtype=np.bool)\n",
    "for i in column2handle:\n",
    "    print(i)\n",
    "    target = X_test[i]\n",
    "    X_test_lil[range(len(X_test)),target.values]=1\n",
    "\n",
    "X_test_lil = X_test_lil.tocsr()\n",
    "save_sparse_csr('data/X_test_csr_'+str(j), X_test_lil)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec3] build SGD , XGB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save csr_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    # here we need to add .npz extension manually\n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_csr = []\n",
    "Y_train = []\n",
    "for i in range(5):\n",
    "    X_train_csr.append(load_sparse_csr('data/X_train_csr_'+str(i)))\n",
    "    Y_train.append(np.load('data/X_train_csr_click_'+str(i)+'.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sec 3-0] Familiar with scikit-learn SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf=SGDClassifier(loss=\"log\",  eta0=1e-5, n_iter=150,\n",
    "                  alpha=1e-5 ,penalty=\"l2\",verbose=1,\n",
    "                  learning_rate=\"constant\") \n",
    "\n",
    "#n_jobs=-1 : number of cpu used, this option is used in one-verse-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 10.74, NNZs: 3551169, Bias: -0.006885, T: 8085793, Avg. loss: 0.401061\n",
      "Total training time: 3.18 seconds.\n",
      "CPU times: user 4.63 s, sys: 128 ms, total: 4.76 s\n",
      "Wall time: 4.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1e-05, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='constant', loss='log', n_iter=150, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "# with using paritial fit \n",
    "clf.partial_fit(X_train_csr_0, Y_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83404027285882787"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_csr_0,Y_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_0_hat=clf.predict_log_proba(X_train_csr_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.exp(y_train_0_hat[0])) # really is log(p_0), log(p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40109270011955145"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # i-th data\n",
    "# likelihood is calculated with pi^(yi)*(1-pi)^(1-yi)\n",
    "# log-likelihood is yi*log(pi)+(1-yi)*log(1-pi)\n",
    "# loss is - sum(log-likelihood)\n",
    "# the log-loss function\n",
    "np.mean(y_train_0_hat[range(len(Y_train_0)),Y_train_0])*(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40085742973102617"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on validation set 1~4\n",
    "y_train_1_hat=clf.predict_log_proba(X_train_csr[1])\n",
    "np.mean(y_train_1_hat[range(len(Y_train_0)),Y_train[1]])*(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sec 3 - 1]  Training with SGD, define evaluate, benchmark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load csr_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    # here we need to add .npz extension manually\n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from files\n",
    "X_train_csr = []\n",
    "Y_train = []\n",
    "for i in range(5):\n",
    "    X_train_csr.append(load_sparse_csr('data/X_train_csr_shuff_'+str(i)))\n",
    "    Y_train.append(np.load('data/X_train_csr_click_shuff_'+str(i)+'.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_evaluate(clf,X_test,Y_test):\n",
    "    Y_hat=clf.predict_log_proba(X_test)\n",
    "    return np.mean(Y_hat[range(len(Y_test)),Y_test])*(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(eta_=1e-5, alpha_=1e-5, fold_train=[0], early_stop=2):\n",
    "    clf = SGDClassifier(loss=\"log\",  eta0=eta_, \n",
    "                        alpha=alpha_ ,penalty=\"l2\",\n",
    "                        average=10,verbose=0)\n",
    "          #learning_rate=\"constant\") \n",
    "    classes = np.array([0,1])\n",
    "    min_loss  = 100\n",
    "    stop_count = 0\n",
    "    best_epoch = 0\n",
    "    for epoch in range(3000):\n",
    "        for idx in fold_train:\n",
    "            clf.partial_fit(X_train_csr[idx], Y_train[idx], \n",
    "                            classes=classes)\n",
    "        if(epoch%30==0):\n",
    "            print('_' * 80)\n",
    "            \n",
    "            print(\"Training epoch: \",epoch)\n",
    "            train_err=[]\n",
    "            for idx in fold_train:\n",
    "                fold_loss = loss_evaluate(clf, X_train_csr[idx], Y_train[idx])\n",
    "                train_err.append(fold_loss)\n",
    "            print(\"** Training err = \",np.mean(train_err))\n",
    "            \n",
    "            print(\"Evaluate on Validation set:\")\n",
    "            testing_err = []\n",
    "            for v in range(0,5):\n",
    "                if v not in fold_train:\n",
    "                    test_loss = loss_evaluate(clf, X_train_csr[v], Y_train[v])\n",
    "                    testing_err.append(test_loss)\n",
    "            w = np.mean(testing_err)\n",
    "            print(\"** Validation err = \",w)\n",
    "            \n",
    "            if w <= min_loss:\n",
    "                best_epoch = epoch\n",
    "                min_loss = w\n",
    "                best_epoch_train_error = np.mean(train_err)\n",
    "            else:\n",
    "                stop_count+=1\n",
    "                if stop_count>=early_stop:\n",
    "                    break\n",
    "    print(\"best epoch is : \", best_epoch,\", with loss : \" ,min_loss)\n",
    "    return (clf, best_epoch, min_loss, best_epoch_train_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "benchmark(eta_=5e-5, alpha_=0, fold_train=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sec 3-2] start training, will use ~15G memory, and one core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stream/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:832: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.predict_proba(X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training err =  2.28168784821\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  2.31549189874\n",
      "________________________________________________________________________________\n",
      "Training epoch:  30\n",
      "** Training err =  0.381079850561\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.40370127638\n",
      "________________________________________________________________________________\n",
      "Training epoch:  60\n",
      "** Training err =  0.367919151147\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.389934222034\n",
      "________________________________________________________________________________\n",
      "Training epoch:  90\n",
      "** Training err =  0.365078821403\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.386858636838\n",
      "________________________________________________________________________________\n",
      "Training epoch:  120\n",
      "** Training err =  0.36403707976\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.385689261899\n",
      "________________________________________________________________________________\n",
      "Training epoch:  150\n",
      "** Training err =  0.363549054013\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.385120236405\n",
      "________________________________________________________________________________\n",
      "Training epoch:  180\n",
      "** Training err =  0.363285423578\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.384800316895\n",
      "________________________________________________________________________________\n",
      "Training epoch:  210\n",
      "** Training err =  0.363129115408\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.384602401208\n",
      "________________________________________________________________________________\n",
      "Training epoch:  240\n",
      "** Training err =  0.363030146021\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.384471350192\n",
      "________________________________________________________________________________\n",
      "Training epoch:  270\n",
      "** Training err =  0.362964390256\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.384380043018\n",
      "________________________________________________________________________________\n",
      "Training epoch:  300\n",
      "** Training err =  0.3629190747\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.38431385506\n",
      "________________________________________________________________________________\n",
      "Training epoch:  330\n",
      "** Training err =  0.362886939861\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.38426433466\n",
      "________________________________________________________________________________\n",
      "Training epoch:  360\n",
      "** Training err =  0.362863645107\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.384226298562\n",
      "________________________________________________________________________________\n",
      "Training epoch:  390\n",
      "** Training err =  0.362846461466\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.384196448582\n",
      "________________________________________________________________________________\n",
      "Training epoch:  420\n",
      "** Training err =  0.362833614001\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.384172600107\n"
     ]
    }
   ],
   "source": [
    "grid_result = dict()\n",
    "grid_lr =[0]\n",
    "grid_alpha = [1e-7]  #[0,1e-5,1e-6]\n",
    "for lr in grid_lr:\n",
    "    for alpha in grid_alpha:\n",
    "        records = benchmark(eta_=lr, alpha_=alpha, fold_train=[0,1,2,3])\n",
    "        grid_result[(lr,alpha)] = records\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#constant update\n",
    "(5e-05, 0): (870, 0.39123732468012118, 0.38637592565738316)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Sec3-2-1] Make CLF as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make clf as input \n",
    "def benchmark_clf(clf, fold_train=[0], early_stop=4, max_epoch=3000, eval_period=30,target_score=None):\n",
    "    classes = np.array([0,1])\n",
    "    min_loss  = 100\n",
    "    best_epoch_train_error = 100\n",
    "    stop_count = 0\n",
    "    best_epoch = 0\n",
    "    for epoch in range(max_epoch):\n",
    "        for idx in fold_train:\n",
    "            clf.partial_fit(X_train_csr[idx], Y_train[idx], \n",
    "                            classes=classes)\n",
    "        if(epoch%eval_period==0):\n",
    "            print('_' * 80)\n",
    "            \n",
    "            print(\"Training epoch: \",epoch)\n",
    "            train_err=[]\n",
    "            for idx in fold_train:\n",
    "                fold_loss = loss_evaluate(clf, X_train_csr[idx], Y_train[idx])\n",
    "                train_err.append(fold_loss)\n",
    "            print(\"** Training err = \",np.mean(train_err))\n",
    "            \n",
    "            print(\"Evaluate on Validation set:\")\n",
    "            testing_err = []\n",
    "            for v in range(0,5):\n",
    "                if v not in fold_train:\n",
    "                    test_loss = loss_evaluate(clf, X_train_csr[v], Y_train[v])\n",
    "                    testing_err.append(test_loss)\n",
    "            w = np.mean(testing_err)\n",
    "            print(\"** Validation err = \",w)\n",
    "            \n",
    "            if target_score!=None:\n",
    "                if(w<= target_score):\n",
    "                    best_epoch = epoch\n",
    "                    min_loss = w\n",
    "                    best_epoch_train_error = np.mean(train_err)\n",
    "                    break;\n",
    "            \n",
    "            if w <= min_loss:\n",
    "                best_epoch = epoch\n",
    "                min_loss = w\n",
    "                best_epoch_train_error = np.mean(train_err)\n",
    "            else:\n",
    "                stop_count+=1\n",
    "                if stop_count>=early_stop:\n",
    "                    break\n",
    "    print(\"best epoch is : \", best_epoch,\", with loss : \" ,min_loss)\n",
    "    return (best_epoch, min_loss, best_epoch_train_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training epoch:  0\n",
      "** Training err =  1.03571012877\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  1.06232216986\n",
      "________________________________________________________________________________\n",
      "Training epoch:  10\n",
      "** Training err =  0.403320950663\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.410268767415\n",
      "________________________________________________________________________________\n",
      "Training epoch:  20\n",
      "** Training err =  0.389707412514\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.396863125692\n"
     ]
    }
   ],
   "source": [
    "grid_result = dict()\n",
    "grid_lr =[0]\n",
    "grid_alpha = [3e-7]  #[0,1e-5,1e-6]\n",
    "for lr in grid_lr:\n",
    "    for alpha in grid_alpha:\n",
    "        clf = SGDClassifier(loss=\"log\", eta0=lr, l1_ratio=0.15,\n",
    "                            alpha=alpha, penalty=\"elasticnet\",\n",
    "                            average=True, verbose=0)\n",
    "        records = benchmark_clf(clf, fold_train=[0,1,2,3],\n",
    "                                max_epoch=5000,target_score=0.391,\n",
    "                                eval_period=10,early_stop=2)\n",
    "        grid_result[(lr,alpha)] = records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (eta=5e-5, alpha=0), epoch:295 loss:0.3962 (train on fold0, test on fold1~4)\n",
    "# (eta=5e-5, alpha=0), epoch:380 loss:0.3933 (train on fold0~3, test on fold4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [Sec 3-2-3] Let's Test on Testing Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test=load_sparse_csr('data/X_test_csr_0')\n",
    "y_hat=clf.predict_proba(X_test)\n",
    "\n",
    "import pandas as pd\n",
    "sampleSubmission = pd.read_csv(\"data/sampleSubmission\")\n",
    "#sampleSubmission.click = (y_site+y_app)/2\n",
    "sampleSubmission.click = y_hat[:,1]#*0.95\n",
    "# with a naive feature hashing (0.408, 1142/1604)\n",
    "sampleSubmission.to_csv(\"md_hash_logisitc_039098.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sampleSubmission = pd.read_csv(\"md_hash_logisitc_03813.csv\")\n",
    "#sampleSubmission.click = (y_site+y_app)/2\n",
    "sampleSubmission.click = sampleSubmission.click*0.98\n",
    "# with a naive feature hashing (0.408, 1142/1604)\n",
    "sampleSubmission.to_csv(\"md_hash_logisitc_03813_095.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Sec3-2-4] five fold average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test=load_sparse_csr('data/X_test_csr_0')\n",
    "y_hat_list=[]\n",
    "alpha = 3e-7\n",
    "for i in range(5):\n",
    "    clf = SGDClassifier(loss=\"log\", l1_ratio=0.15,\n",
    "                        alpha=alpha, penalty=\"elasticnet\",power_t=1e-5,\n",
    "                        average=True, verbose=0)\n",
    "    benchmark_clf(clf,\n",
    "                  max_epoch=100,\n",
    "                  eval_period=5,\n",
    "                  target_score=0.391,\n",
    "                  fold_train=[j for j in range(5) if i!=j],\n",
    "                 )\n",
    "    y_hat_list.append(clf.predict_proba(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat=np.mean(y_hat_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sampleSubmission = pd.read_csv(\"data/sampleSubmission\")\n",
    "#sampleSubmission.click = (y_site+y_app)/2\n",
    "sampleSubmission.click = y_hat[:,1]\n",
    "# with a naive feature hashing (0.408, 1142/1604)\n",
    "sampleSubmission.to_csv(\"3e-7_bagging_03910.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def f_1(x):\n",
    "    if(x>1):\n",
    "        return 1\n",
    "    else: \n",
    "        return x\n",
    "sampleSubmission = pd.read_csv(\"3e-7_bagging_03910.csv\")\n",
    "#sampleSubmission.click = (y_site+y_app)/2\n",
    "sampleSubmission.click = sampleSubmission.click*1.02\n",
    "sampleSubmission.click = sampleSubmission.click.apply(f_1)\n",
    "\n",
    "# with a naive feature hashing (0.408, 1142/1604)\n",
    "sampleSubmission.to_csv(\"3e-7_bagging_03910_102.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sec 3-3] Xgboost support with sparse matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load csr_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    # here we need to add .npz extension manually\n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_csr = []\n",
    "Y_train = []\n",
    "for i in range(5):\n",
    "    X_train_csr.append(load_sparse_csr('data/X_train_csr_'+str(i)))\n",
    "    Y_train.append(np.load('data/X_train_csr_click_'+str(i)+'.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train_csr[0], label = Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deval = xgb.DMatrix(X_train_csr[1], label = Y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "watchlist  = [(dtrain,'train'),(deval,'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {'max_depth':6,\n",
    "         'eta':0.05,\n",
    "         'objective': 'binary:logistic',\n",
    "         'eval_metric': 'logloss'\n",
    "        }\n",
    "param['nthread'] = 4\n",
    "param['subsample'] = 0.3\n",
    "param['colsample_bytree'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, 8000, watchlist , early_stopping_rounds=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
