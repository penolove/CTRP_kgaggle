{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sec1] create Sparse Matrix with pd get_dummies (failure)\n",
    "\n",
    "fail due to >pd.get_dummies is extremely unefficientcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X_train = pd.read_csv(\"data/train.csv\")\n",
    "X_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=X_test.columns.values[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling col :  C1\n",
      "handling col :  banner_pos\n",
      "handling col :  site_id\n",
      "handling col :  site_domain\n",
      "handling col :  site_category\n",
      "handling col :  app_id\n",
      "handling col :  app_domain\n",
      "handling col :  app_category\n",
      "handling col :  device_id\n",
      "handling col :  device_ip\n",
      "handling col :  device_model\n",
      "handling col :  device_type\n",
      "handling col :  device_conn_type\n",
      "handling col :  C14\n",
      "handling col :  C15\n",
      "handling col :  C16\n",
      "handling col :  C17\n",
      "handling col :  C18\n",
      "handling col :  C19\n",
      "handling col :  C20\n",
      "handling col :  C21\n"
     ]
    }
   ],
   "source": [
    "features = X_test.columns.values[2:]\n",
    "for column in features:\n",
    "    print(\"handling col : \",column )\n",
    "    categories = pd.concat([X_train[column],X_test[column]]).unique()\n",
    "    X_train[column] = X_train[column].astype('category', categories=categories)\n",
    "    X_test[column] = X_test[column].astype('category', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"Train_cate\")\n",
    "X_test.to_pickle(\"Test_cate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=pd.read_pickle(\"Train_cate\")\n",
    "X_test=pd.read_pickle(\"Test_cate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into 5 folds\n",
    "import numpy as np\n",
    "def n_fold_split(df, k, seed=5566):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df)\n",
    "    fold_size = int(m/k)\n",
    "    fold_list = []\n",
    "    for i in range(k):\n",
    "        if i!=(k-1):\n",
    "            target_range=range(i*fold_size,(i+1)*fold_size)\n",
    "        else:\n",
    "            target_range=range(i*fold_size,m)\n",
    "        current_df = df.iloc[perm[target_range]]\n",
    "        fold_list.append(current_df)\n",
    "    return fold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085795\n"
     ]
    }
   ],
   "source": [
    "X_train_list = n_fold_split(X_train,5)\n",
    "for i in X_train_list:\n",
    "    print(\"batchsize:\",len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=X_test.columns.values[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C1', 'banner_pos', 'site_id', 'site_domain', 'site_category',\n",
       "       'app_id', 'app_domain', 'app_category', 'device_id', 'device_ip',\n",
       "       'device_model', 'device_type', 'device_conn_type', 'C14', 'C15',\n",
       "       'C16', 'C17', 'C18', 'C19', 'C20', 'C21'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.get_dummies(X_test, columns=features, sparse=True)\n",
    "# extremely slowly and run-out memory (even with only Test dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec2] manually build csr matrix, and fit xgb/SGD model\n",
    "what we want -> a very large matrix with all category variable were translated to dummy\n",
    "\n",
    "and sloving with logistic regression\n",
    "\n",
    "## [Sec2-1]   load df which were built in FF ipynb,  and split it into 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "X_train = pd.read_pickle('train_df_hash')\n",
    "X_test = pd.read_pickle('test_df_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column2handle = X_train.columns.values[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split to 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into 5 folds\n",
    "import numpy as np\n",
    "def n_fold_split(df, k, seed=5566):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df)\n",
    "    fold_size = int(m/k)\n",
    "    fold_list = []\n",
    "    for i in range(k):\n",
    "        if i!=(k-1):\n",
    "            target_range=range(i*fold_size,(i+1)*fold_size)\n",
    "        else:\n",
    "            target_range=range(i*fold_size,m)\n",
    "        current_df = df.iloc[perm[target_range]]\n",
    "        fold_list.append(current_df)\n",
    "    return fold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085793\n",
      "batchsize: 8085795\n"
     ]
    }
   ],
   "source": [
    "X_train_list = n_fold_split(X_train,5)\n",
    "for i in X_train_list:\n",
    "    print(\"batchsize:\",len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Sec 2-1-0] Test the format of sparse matrix\n",
    "said dok_matrix/lil_matrix is good at constructing sparse matrices.\n",
    "\n",
    "give a naive try to this two type and save it as csr_matrix\n",
    "\n",
    "[REF](http://m.blog.csdn.net/pipisorry/article/details/41762945)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on dok_matrix\n",
    "X_train_dok = dok_matrix((8085793, 10269200), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 51s, sys: 6.21 s, total: 8min 57s\n",
      "Wall time: 9min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in column2handle:\n",
    "    target = X_train_list[0][i]\n",
    "    X_train_dok[range(8085793),target.values]=1\n",
    "X_train_dok = X_train_dok.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on lil_matrix\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lil = lil_matrix((8085793, 10269200), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.6 s, sys: 6.13 s, total: 1min\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in column2handle:\n",
    "    target = X_train_list[0][i]\n",
    "    X_train_lil[range(8085793),target.values]=1\n",
    "X_train_lil = X_train_lil.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7421"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [sec 2-1-1] filling hash values into sparse matrix \n",
    "### using lil_matrix is much more faster for assigning values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save/load function for  csr_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "def save_sparse_csr(filename, array):\n",
    "    # note that .npz extension is added automatically\n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    # here we need to add .npz extension manually\n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "C1_hash\n",
      "banner_pos_hash\n",
      "site_id_hash\n",
      "site_domain_hash\n",
      "site_category_hash\n",
      "app_id_hash\n",
      "app_domain_hash\n",
      "app_category_hash\n",
      "device_id_hash\n",
      "device_ip_hash\n",
      "device_model_hash\n",
      "device_type_hash\n",
      "device_conn_type_hash\n",
      "C14_hash\n",
      "C15_hash\n",
      "C16_hash\n",
      "C17_hash\n",
      "C18_hash\n",
      "C19_hash\n",
      "C20_hash\n",
      "C21_hash\n",
      "weekday_hash\n",
      "hour__hash\n",
      "CPU times: user 5min 12s, sys: 35.5 s, total: 5min 48s\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for j in range(5):\n",
    "    X_train_lil = lil_matrix(( len(X_train_list[j]), 10269200), dtype=np.bool)\n",
    "    gc.collect()\n",
    "    for i in column2handle:\n",
    "        print(i)\n",
    "        target = X_train_list[j][i]\n",
    "        X_train_lil[range(len(X_train_list[j])),target.values]=1\n",
    "        \n",
    "    X_train_lil = X_train_lil.tocsr()\n",
    "    save_sparse_csr('data/X_train_csr_'+str(j), X_train_lil)\n",
    "    np.save('data/X_train_csr_click_'+str(j), X_train_list[j].click.values)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec3] build SGD , XGB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save csr_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    # here we need to add .npz extension manually\n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_csr = []\n",
    "Y_train = []\n",
    "for i in range(5):\n",
    "    X_train_csr.append(load_sparse_csr('data/X_train_csr_'+str(i)))\n",
    "    Y_train.append(np.load('data/X_train_csr_click_'+str(i)+'.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sec 3-0] Familiar with scikit-learn SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf=SGDClassifier(loss=\"log\",  eta0=1e-5, n_iter=150,\n",
    "                  alpha=1e-5 ,penalty=\"l2\",verbose=1,\n",
    "                  learning_rate=\"constant\") \n",
    "\n",
    "#n_jobs=-1 : number of cpu used, this option is used in one-verse-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 10.74, NNZs: 3551169, Bias: -0.006885, T: 8085793, Avg. loss: 0.401061\n",
      "Total training time: 3.18 seconds.\n",
      "CPU times: user 4.63 s, sys: 128 ms, total: 4.76 s\n",
      "Wall time: 4.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1e-05, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=1e-05, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='constant', loss='log', n_iter=150, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "# with using paritial fit \n",
    "clf.partial_fit(X_train_csr_0, Y_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83404027285882787"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_csr_0,Y_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_0_hat=clf.predict_log_proba(X_train_csr_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.exp(y_train_0_hat[0])) # really is log(p_0), log(p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40109270011955145"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # i-th data\n",
    "# likelihood is calculated with pi^(yi)*(1-pi)^(1-yi)\n",
    "# log-likelihood is yi*log(pi)+(1-yi)*log(1-pi)\n",
    "# loss is - sum(log-likelihood)\n",
    "# the log-loss function\n",
    "np.mean(y_train_0_hat[range(len(Y_train_0)),Y_train_0])*(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40085742973102617"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on validation set 1~4\n",
    "y_train_1_hat=clf.predict_log_proba(X_train_csr[1])\n",
    "np.mean(y_train_1_hat[range(len(Y_train_0)),Y_train[1]])*(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sec 3 - 1]  Training with SGD, define evaluate, benchmark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load csr_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    # here we need to add .npz extension manually\n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading data from files\n",
    "X_train_csr = []\n",
    "Y_train = []\n",
    "for i in range(5):\n",
    "    X_train_csr.append(load_sparse_csr('data/X_train_csr_'+str(i)))\n",
    "    Y_train.append(np.load('data/X_train_csr_click_'+str(i)+'.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_evaluate(clf,X_test,Y_test):\n",
    "    Y_hat=clf.predict_log_proba(X_test)\n",
    "    return np.mean(Y_hat[range(len(Y_test)),Y_test])*(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(eta_=1e-5, alpha_=1e-5, fold_train=[0], early_stop=2):\n",
    "    clf = SGDClassifier(loss=\"log\",  eta0=eta_, \n",
    "          alpha=alpha_ ,penalty=\"l2\",verbose=0,\n",
    "          learning_rate=\"constant\") \n",
    "    classes = np.array([0,1])\n",
    "    min_loss  = 100\n",
    "    stop_count = 0\n",
    "    best_epoch = 0\n",
    "    for epoch in range(1000):\n",
    "        for idx in fold_train:\n",
    "            clf.partial_fit(X_train_csr[idx], Y_train[idx], \n",
    "                            classes=classes)\n",
    "        if(epoch%10==0):\n",
    "            print('_' * 80)\n",
    "            print(\"Training epoch: \",epoch)\n",
    "            train_err=[]\n",
    "            for idx in fold_train:\n",
    "                fold_loss = loss_evaluate(clf, X_train_csr[idx], Y_train[idx])\n",
    "                train_err.append(fold_loss)\n",
    "            \n",
    "            print(\"** Training err = \",np.mean(train_err))\n",
    "            print(\"Evaluate on Validation set:\")\n",
    "            testing_err = []\n",
    "            for v in range(0,5):\n",
    "                if v not in fold_train:\n",
    "                    test_loss = loss_evaluate(clf, X_train_csr[v], Y_train[v])\n",
    "                    testing_err.append(test_loss)\n",
    "            w = np.mean(testing_err)\n",
    "            print(\"** Validation err = \",w)\n",
    "            if w <= min_loss:\n",
    "                best_epoch = epoch\n",
    "                min_loss = w\n",
    "                best_epoch_train_error = np.mean(train_err)\n",
    "            else:\n",
    "                stop_count+=1\n",
    "                if stop_count>=early_stop:\n",
    "                    break\n",
    "    print(\"best epoch is : \", best_epoch,\", with loss : \" ,min_loss)\n",
    "    return (epoch, min_loss, best_epoch_train_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training epoch:  0\n",
      "** Training err =  0.407077696484\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.407167750835\n",
      "________________________________________________________________________________\n",
      "Training epoch:  10\n",
      "** Training err =  0.399717436783\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.399915028708\n",
      "________________________________________________________________________________\n",
      "Training epoch:  20\n",
      "** Training err =  0.398233745033\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.398529681716\n",
      "________________________________________________________________________________\n",
      "Training epoch:  30\n",
      "** Training err =  0.397432463073\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.397808687287\n",
      "________________________________________________________________________________\n",
      "Training epoch:  40\n",
      "** Training err =  0.396803993206\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.397264137772\n",
      "________________________________________________________________________________\n",
      "Training epoch:  50\n",
      "** Training err =  0.396402208518\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.396923954742\n",
      "________________________________________________________________________________\n",
      "Training epoch:  60\n",
      "** Training err =  0.395966345375\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.396575664254\n",
      "________________________________________________________________________________\n",
      "Training epoch:  70\n",
      "** Training err =  0.395627614482\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.396300303317\n",
      "________________________________________________________________________________\n",
      "Training epoch:  80\n",
      "** Training err =  0.395342756118\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.396092238084\n",
      "________________________________________________________________________________\n",
      "Training epoch:  90\n",
      "** Training err =  0.395082310328\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.395891672618\n",
      "________________________________________________________________________________\n",
      "Training epoch:  100\n",
      "** Training err =  0.394829612113\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.395699707731\n",
      "________________________________________________________________________________\n",
      "Training epoch:  110\n",
      "** Training err =  0.394640611578\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.39558001056\n",
      "________________________________________________________________________________\n",
      "Training epoch:  120\n",
      "** Training err =  0.394385471117\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.395392037059\n",
      "________________________________________________________________________________\n",
      "Training epoch:  130\n",
      "** Training err =  0.394175981619\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.395244792235\n",
      "________________________________________________________________________________\n",
      "Training epoch:  140\n",
      "** Training err =  0.393989008357\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.39512503026\n",
      "________________________________________________________________________________\n",
      "Training epoch:  150\n",
      "** Training err =  0.393809024529\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.395006157636\n",
      "________________________________________________________________________________\n",
      "Training epoch:  160\n",
      "** Training err =  0.393636647783\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.394895146107\n",
      "________________________________________________________________________________\n",
      "Training epoch:  170\n",
      "** Training err =  0.393468404769\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.394784749185\n",
      "________________________________________________________________________________\n",
      "Training epoch:  180\n",
      "** Training err =  0.393302315415\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.394678676865\n",
      "________________________________________________________________________________\n",
      "Training epoch:  190\n",
      "** Training err =  0.393165128111\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.394611310735\n",
      "________________________________________________________________________________\n",
      "Training epoch:  200\n",
      "** Training err =  0.392993585754\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.394493859107\n",
      "________________________________________________________________________________\n",
      "Training epoch:  210\n",
      "** Training err =  0.392852303297\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.394407543079\n",
      "________________________________________________________________________________\n",
      "Training epoch:  220\n",
      "** Training err =  0.392702041571\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.394318261558\n",
      "________________________________________________________________________________\n",
      "Training epoch:  230\n",
      "** Training err =  0.392585163492\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.394266198395\n",
      "________________________________________________________________________________\n",
      "Training epoch:  240\n",
      "** Training err =  0.392437060647\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.394169832117\n",
      "________________________________________________________________________________\n",
      "Training epoch:  250\n",
      "** Training err =  0.392358695974\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.39415681528\n",
      "________________________________________________________________________________\n",
      "Training epoch:  260\n",
      "** Training err =  0.392151952021\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393997814368\n",
      "________________________________________________________________________________\n",
      "Training epoch:  270\n",
      "** Training err =  0.392032979849\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393934235263\n",
      "________________________________________________________________________________\n",
      "Training epoch:  280\n",
      "** Training err =  0.391897266319\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393859671894\n",
      "________________________________________________________________________________\n",
      "Training epoch:  290\n",
      "** Training err =  0.391780337417\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393787656187\n",
      "________________________________________________________________________________\n",
      "Training epoch:  300\n",
      "** Training err =  0.39166257315\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393729555954\n",
      "________________________________________________________________________________\n",
      "Training epoch:  310\n",
      "** Training err =  0.391532789779\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393653989469\n",
      "________________________________________________________________________________\n",
      "Training epoch:  320\n",
      "** Training err =  0.391406385073\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393592090592\n",
      "________________________________________________________________________________\n",
      "Training epoch:  330\n",
      "** Training err =  0.391315767053\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393549909752\n",
      "________________________________________________________________________________\n",
      "Training epoch:  340\n",
      "** Training err =  0.391164967153\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393455877983\n",
      "________________________________________________________________________________\n",
      "Training epoch:  350\n",
      "** Training err =  0.391072946296\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393430014894\n",
      "________________________________________________________________________________\n",
      "Training epoch:  360\n",
      "** Training err =  0.390961568386\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393359871573\n",
      "________________________________________________________________________________\n",
      "Training epoch:  370\n",
      "** Training err =  0.39084457975\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.39330078837\n",
      "________________________________________________________________________________\n",
      "Training epoch:  380\n",
      "** Training err =  0.390828741839\n",
      "** Evaluate on Validation set:\n",
      "Validation err =  0.393351037766\n",
      "stoping epoch is :  380 , with loss :  0.39330078837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(380, 0.39330078837013349)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(eta_=5e-5, alpha_=0, fold_train=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (eta=5e-5, alpha=0), epoch:295 loss:0.3962 (train on fold0, test on fold1~4)\n",
    "# (eta=5e-5, alpha=0), epoch:380 loss:0.3933 (train on fold0~3, test on fold4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sec 3-2] start training, will use ~15G memory, and one core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training epoch:  0\n",
      "** Training err =  0.407151195599\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.407189635883\n",
      "________________________________________________________________________________\n",
      "Training epoch:  10\n",
      "** Training err =  0.399750971706\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.399899650102\n",
      "________________________________________________________________________________\n",
      "Training epoch:  20\n",
      "** Training err =  0.398333894884\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.398591777134\n",
      "________________________________________________________________________________\n",
      "Training epoch:  30\n",
      "** Training err =  0.397441025824\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.397777307659\n",
      "________________________________________________________________________________\n",
      "Training epoch:  40\n",
      "** Training err =  0.396848674031\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.397264463114\n",
      "________________________________________________________________________________\n",
      "Training epoch:  50\n",
      "** Training err =  0.396389485617\n",
      "Evaluate on Validation set:\n",
      "** Validation err =  0.396877111891\n"
     ]
    }
   ],
   "source": [
    "grid_result = dict()\n",
    "grid_lr =[5e-5,1e-5,1e-6]\n",
    "grid_alpha = [0]  #[0,1e-5,1e-6]\n",
    "for lr in grid_lr:\n",
    "    for alpha in grid_alpha:\n",
    "        records = benchmark(eta_=lr, alpha_=alpha, fold_train=[0,1,2,3])\n",
    "        grid_result[(lr,alpha)] = records\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
